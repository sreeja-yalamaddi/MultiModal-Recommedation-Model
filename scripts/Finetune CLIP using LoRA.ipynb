{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7a7ccb-fc66-4b29-8fe7-f1737b75b8d5",
   "metadata": {},
   "source": [
    "!pip install transformers accelerate peft datasets torchvision bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c49b19d-e36d-4bdd-b305-58a40cbe5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from utils import load_and_clean_data, get_model, save_model_and_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48ef4-1b17-4346-b247-e44c1b412115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "CSV_PATH      = \"meta_data_beauty.csv\"\n",
    "SAVE_DIR      = \"artifacts_lora_beauty/\"\n",
    "BATCH_SIZE    = 128 # batch size for training\n",
    "NUM_EPOCHS    = 20 # 10 gave best results\n",
    "LR            = 2e-5\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME    = \"openai/clip-vit-base-patch32\"\n",
    "os.makedirs(SAVE_DIR,     exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e64f23-5767-4f8f-b7c6-890ab8e45f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── DATASET +  collate_fn ────────────────────────────────────────────────\n",
    "class ProductCLIPDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts     = df[\"product_text\"].tolist()\n",
    "        self.urls      = df[\"product_image_url\"].tolist()\n",
    "        proc = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "        self.tokenizer       = proc.tokenizer\n",
    "        self.image_processor = proc.image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url  = self.urls[idx]\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=5)\n",
    "            img  = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        except:\n",
    "            img  = Image.new(\"RGB\", (224,224), \"white\")\n",
    "        return {\"text\": text, \"image\": img}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts  = [ex[\"text\"]  for ex in batch]\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        imgs = self.image_processor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      tok.input_ids,\n",
    "            \"attention_mask\": tok.attention_mask,\n",
    "            \"pixel_values\":   imgs\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1fa4bd6-dade-4509-b149-2d675765a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = load_and_clean_data(CSV_PATH)\n",
    "#df_train = df.sample(20000)\n",
    "df_train = pd.read_csv(CSV_PATH).sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e3ea0-a544-4647-98b4-6d32974d2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(text=texts,\n",
    "                    images=images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True)\n",
    "    return enc\n",
    "\n",
    "dataset = ProductCLIPDataset(df_train)\n",
    "loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b675524a-2537-4616-aba4-64fbf4c742bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(approach=\"lora\", save_dir=None)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d95476-700e-4c6d-acdc-2c781c6fb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████████████| 157/157 [1:21:49<00:00, 31.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 4.3760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████| 157/157 [1:08:47<00:00, 26.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 4.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████| 157/157 [1:06:32<00:00, 25.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 4.1410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|███████████████████████████| 157/157 [1:05:31<00:00, 25.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 4.1139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|███████████████████████████| 157/157 [1:03:49<00:00, 24.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 4.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|███████████████████████████| 157/157 [1:05:57<00:00, 25.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 4.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|███████████████████████████| 157/157 [1:04:31<00:00, 24.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 4.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|███████████████████████████| 157/157 [1:03:47<00:00, 24.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg loss: 4.0624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|███████████████████████████| 157/157 [1:02:38<00:00, 23.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg loss: 4.0551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████████████████████| 157/157 [1:01:19<00:00, 23.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg loss: 4.0516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████████████████████| 157/157 [1:03:09<00:00, 24.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 avg loss: 4.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████████████████████| 157/157 [1:00:24<00:00, 23.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 avg loss: 4.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|████████████████████████████| 157/157 [59:56<00:00, 22.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 avg loss: 4.0418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|████████████████████████████| 157/157 [59:35<00:00, 22.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 avg loss: 4.0367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|████████████████████████████| 157/157 [59:05<00:00, 22.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 avg loss: 4.0360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|████████████████████████████| 157/157 [59:23<00:00, 22.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 avg loss: 4.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|████████████████████████████| 157/157 [59:32<00:00, 22.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 avg loss: 4.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|████████████████████████████| 157/157 [59:34<00:00, 22.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 avg loss: 4.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|████████████████████████████| 157/157 [59:54<00:00, 22.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 avg loss: 4.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████████████████████| 157/157 [1:01:00<00:00, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 avg loss: 4.0245\n",
      "75984.39484095573\n",
      "Training time taken : 1266.4 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move all to device\n",
    "        batch = {k:v.to(DEVICE) for k,v in batch.items()}\n",
    "\n",
    "        # 1) Get embeddings\n",
    "        text_embs  = model.get_text_features(**{k:batch[k] for k in [\"input_ids\",\"attention_mask\"]})\n",
    "        image_embs = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "        # 2) Normalize\n",
    "        text_embs  = F.normalize(text_embs,  p=2, dim=-1)\n",
    "        image_embs = F.normalize(image_embs, p=2, dim=-1)\n",
    "\n",
    "        # 3) Similarity logits\n",
    "        logits_per_text  = text_embs @ image_embs.t()\n",
    "        logits_per_image = logits_per_text.t()\n",
    "\n",
    "        # 4) Contrastive loss\n",
    "        B = logits_per_text.size(0)\n",
    "        labels = torch.arange(B, device=DEVICE)\n",
    "        loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "        loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "        loss = (loss_t2i + loss_i2t) / 2\n",
    "\n",
    "        # 5) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg:.4f}\")\n",
    "print(time.time() -  start_time)\n",
    "print(\"Training time taken :\" , round(time.time() -  start_time)/60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a43930b1-19b3-441f-91bf-861a4b96501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time taken : 19.96\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time taken :\" , round(1197.38/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "269f2c8a-5bcc-41a4-9fbd-d6d680f97aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_processor(model, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852a979-ef2e-44fb-aea4-241d3e349849",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625caccd-b6fb-452c-a725-fce2e6c0e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ad5162-2a7e-4077-832f-c54c8b6a3b59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxm265/.conda/envs/test/lib/python3.13/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/yxm265/.conda/envs/test/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForFeatureExtraction(\n",
       "  (base_model): LoraModel(\n",
       "    (model): CLIPModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 512)\n",
       "          (position_embedding): Embedding(77, 512)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "          (position_embedding): Embedding(50, 768)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPSdpaAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "      (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Load Model & Generate Embeddings ────────────────────────────────────\n",
    "tuned_model = get_model(approach=\"lora\", save_dir=SAVE_DIR)\n",
    "tuned_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c449c0-b2f3-46c0-a4ca-397ab47458a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # **All tensors here are on CPU**—no .to(device)!\n",
    "    return {\n",
    "        \"input_ids\":      enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"pixel_values\":   enc[\"pixel_values\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dbd1769-cf56-4731-8871-b35b0dcc92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataset, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Returns two CPU tensors: text_embs [N, D], image_embs [N, D].\n",
    "    \"\"\"\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,            # collate_fn returns CPU\n",
    "        collate_fn=dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    model.eval().to(DEVICE)\n",
    "    all_text, all_image = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating embeddings\"):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # 4️Get features\n",
    "            t = model.get_text_features(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            i = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "            # 5️Normalize & collect on CPU\n",
    "            all_text.append(F.normalize(t,  dim=-1).cpu())\n",
    "            all_image.append(F.normalize(i, dim=-1).cpu())\n",
    "\n",
    "    text_embs  = torch.cat(all_text,  dim=0)\n",
    "    image_embs = torch.cat(all_image, dim=0)\n",
    "    return text_embs, image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724e66a1-f4d5-47b7-bd8b-607d910da35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "SAVE_DIR      = \"artifacts_lora_beauty/\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccda90-757e-45ee-8f73-34d2360a3937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f763f1-4586-45fe-b63e-d830e238a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████████| 1760/1760 [1:25:38<00:00,  2.92s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = ProductCLIPDataset(df_full)\n",
    "text_embs, image_embs = generate_embeddings(tuned_model, dataset, batch_size=64, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4854ec4d-72c2-42d6-8a69-69f0c894e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shapes: torch.Size([112578, 512]) torch.Size([112578, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shapes:\", text_embs.shape, image_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe32696-7259-4dcd-a5c8-2eeee9f05b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to: artifacts_lora_fash/faiss.index\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(text_embs, image_embs, SAVE_DIR)\n",
    "\n",
    "combined = F.normalize((text_embs + image_embs) / 2, dim=-1)\n",
    "index_path = build_faiss_index(combined, SAVE_DIR)\n",
    "print(\"FAISS index saved to:\", index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b035f-d29e-496a-9427-91ac9b9f15b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
