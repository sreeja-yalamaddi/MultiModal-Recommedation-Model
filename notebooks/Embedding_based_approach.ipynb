{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74eb5a5-c8c2-4eb9-b15b-03b5f6eaa8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers datasets peft faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da116ab-808a-48f0-9372-54a1147b1baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import faiss\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf8f185-a295-4ff9-8d42-db7a058c3f62",
   "metadata": {},
   "source": [
    "### DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fe54aa-2f6d-45e8-bfae-c4792a9f6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata split from Amazon Reviews 2023\n",
    "product_meta_data = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_All_Beauty\", split=\"full\", trust_remote_code=True)\n",
    "df_meta = pd.DataFrame.from_records(product_meta_data).add_prefix(\"product_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bad520-385b-4939-8814-ca3e2483d990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112590, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dfe5e8d-5c3f-4e9e-892e-6e6c231f3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\",  split=\"full\", trust_remote_code=True)\n",
    "df_review = pd.DataFrame.from_records(review_data).add_prefix(\"review_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff54099e-52e4-492a-9070-596b8e3a77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_clean = df_meta[df_meta['product_parent_asin'].isin(df_review['review_parent_asin'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e50137f-0d20-4e3d-ad68-ef9d9f40a53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1269787/2919265753.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  .applymap(lambda x: x is None).any(axis=1)]\n"
     ]
    }
   ],
   "source": [
    "# Columns to clean\n",
    "target_cols = ['product_title', 'product_description', 'product_images']\n",
    "\n",
    "# 1) Drop NaNs and literal None’s\n",
    "df_meta_clean = df_meta_clean.dropna(subset=target_cols)\n",
    "df_meta_clean = df_meta_clean[~df_meta_clean[target_cols]\n",
    "    .applymap(lambda x: x is None).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0527e89-314e-4f29-b583-729a24cf6ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112565, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47579524-17e2-4990-8659-53a26f300352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning: 112553 rows\n"
     ]
    }
   ],
   "source": [
    "# 2) Define what an “invalid” string is\n",
    "invalid_strs = {'', 'n/a', 'none', 'na'}\n",
    "\n",
    "# 3) Validator for product_images\n",
    "def images_valid(img_dict):\n",
    "    if not isinstance(img_dict, dict):\n",
    "        return False\n",
    "    # only consider these keys for actual URLs\n",
    "    for key in ('hi_res', 'large', 'thumb'):\n",
    "        urls = img_dict.get(key, [])\n",
    "        if not isinstance(urls, (list, tuple)):\n",
    "            continue\n",
    "        for url in urls:\n",
    "            if isinstance(url, str) and url.strip().lower() not in invalid_strs:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# 4) General validator for text fields\n",
    "def text_valid(x):\n",
    "    return isinstance(x, str) and x.strip().lower() not in invalid_strs\n",
    "\n",
    "# 5) Apply validators\n",
    "#   - title & description must pass text_valid\n",
    "#   - images must pass images_valid\n",
    "df_meta_clean = df_meta_clean[\n",
    "    df_meta_clean['product_title'].apply(text_valid) &\n",
    "    #df_meta_clean['product_description'].apply(text_valid) &\n",
    "    df_meta_clean['product_images'].apply(images_valid)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(f\"After cleaning: {len(df_meta_clean)} rows\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3217d48-6dbf-4cb7-9625-90dc831759d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset with extracted image URLs: 112553 rows\n"
     ]
    }
   ],
   "source": [
    "def extract_first_valid_image(images_dict):\n",
    "    if not isinstance(images_dict, dict):\n",
    "        return None\n",
    "\n",
    "    # Keys we care about, in order of preference\n",
    "    image_keys = ['hi_res', 'large', 'thumb']\n",
    "    invalid_strs = {'', 'none', 'n/a', 'na'}\n",
    "\n",
    "    for key in image_keys:\n",
    "        urls = images_dict.get(key, [])\n",
    "        if not isinstance(urls, list):\n",
    "            continue\n",
    "        for url in urls:\n",
    "            if isinstance(url, str) and url.strip().lower() not in invalid_strs:\n",
    "                return url.strip()\n",
    "    return None\n",
    "\n",
    "# Apply it to create a new column: product_image_url\n",
    "df_meta_clean['product_image_url'] = df_meta_clean['product_images'].apply(extract_first_valid_image)\n",
    "\n",
    "# Optional: drop rows where no valid image could be extracted (just in case)\n",
    "df_meta_clean = df_meta_clean[df_meta_clean['product_image_url'].notnull()].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset with extracted image URLs: {len(df_meta_clean)} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d4fdaf9-0a83-4467-8a79-88648123c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_description(desc):\n",
    "    if isinstance(desc, list):\n",
    "        return \" \".join([d.strip() for d in desc if isinstance(d, str)]).strip()\n",
    "    elif isinstance(desc, str):\n",
    "        return desc.strip()\n",
    "    return \"\"\n",
    "\n",
    "df_meta_clean['product_description'] = df_meta_clean['product_description'].apply(flatten_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21845f86-d43d-4062-bc3a-363137ea8e3c",
   "metadata": {},
   "source": [
    "## EMBEDDING APPROACH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0baef119-de72-4a90-8283-b07ac1381a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import CLIPProcessor,CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f87639b-6779-48be-aa61-35fb6d7c20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_and_clean_data,get_model,generate_embeddings,save_embeddings,build_faiss_index\n",
    "SAVE_DIR = \"artifacts_zeroshot\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa3de6f-3fac-4bcb-bfb1-3bbced565623",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_data = load_and_clean_data(\"product_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f580e949-db67-484f-8685-82d09a54f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "class ProductCLIPDataset(Dataset):\n",
    "    def __init__(self, df, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.texts = df[\"product_text\"].tolist()\n",
    "        self.urls = df[\"product_image_url\"].tolist()\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url = self.urls[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "        except:\n",
    "            # Fallback image in case of failure\n",
    "            image = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts = [ex[\"text\"] for ex in batch]\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "\n",
    "        # Tokenize text\n",
    "        tokenized = self.processor.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Process images\n",
    "        image_inputs = self.processor.image_processor(\n",
    "            images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": tokenized[\"input_ids\"],\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "            \"pixel_values\": image_inputs[\"pixel_values\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ad9152e-e561-402b-95cd-86744e5fe58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: CLIPModel\n"
     ]
    }
   ],
   "source": [
    "# Load Zero-Shot CLIP Model\n",
    "model_zs = get_model(approach=\"zero_shot\", save_dir=SAVE_DIR)\n",
    "print(\"Loaded model:\", model_zs.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18080aeb-8d3e-4449-b1b7-3680af66e0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Generating embeddings: 100%|██████████████| 2753/2753 [1:14:48<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings: torch.Size([88083, 512]) torch.Size([88083, 512])\n"
     ]
    }
   ],
   "source": [
    "# Generate text & image embeddings\n",
    "dataset = ProductCLIPDataset(prod_data)\n",
    "text_embs, image_embs = generate_embeddings(model_zs, dataset, batch_size=32)\n",
    "print(\"Generated embeddings:\", text_embs.shape, image_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "915088f9-bc51-456f-947e-c443ec40a4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to: artifacts_zeroshot/faiss.index\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(text_embs, image_embs, SAVE_DIR)\n",
    "\n",
    "combined = F.normalize((text_embs + image_embs) / 2, dim=-1)\n",
    "index_path = build_faiss_index(combined, SAVE_DIR)\n",
    "print(\"FAISS index saved to:\", index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7f12f-735b-4c77-a109-d63714a55113",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAMPLE TESTING\n",
    "\n",
    "q_text = \"photo finish Professional airbrush makeup\"\n",
    "q_img  = \"https://temptupro.com/cdn/shop/products/s-one-essential-airbrush-kit-hero_2.jpg?v=1743181132&width=1780\"\n",
    "\n",
    "recs_text, scores_text = unified_query(\n",
    "    input_text=q_text,\n",
    "    input_image_path=None,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Text-only recommendations:\")\n",
    "print(recs_text[[\"product_title\",\"product_image_url\"]], scores_text)\n",
    "\n",
    "recs_img, scores_img = unified_query(\n",
    "    input_text=None,\n",
    "    input_image_path=q_img,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Image-only recommendations:\")\n",
    "print(recs_img[[\"product_title\",\"product_image_url\"]], scores_img)\n",
    "\n",
    "recs_both, scores_both = unified_query(\n",
    "    input_text=q_text,\n",
    "    input_image_path=q_img,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Text+Image recommendations:\")\n",
    "print(recs_both[[\"product_title\",\"product_image_url\"]], scores_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0a2b71-7e0b-4271-9a39-c6b421b9bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your queries\n",
    "df_queries = pd.read_excel(\"Amazon_recom_queries.xlsx\")\n",
    "\n",
    "# 2) Group by unique Queries, collecting Amazon’s ground-truth lists\n",
    "amazon_grouped = df_queries.groupby(\"Queries\").agg({\n",
    "    \"Product_title\":       list,\n",
    "    \"Product_description\": list,\n",
    "    \"Product_link\":        list,\n",
    "    \"Image_link\":          list\n",
    "}).reset_index()\n",
    "\n",
    "# 3) Prepare columns to hold your model’s recommendations\n",
    "amazon_grouped[\"Model_rec_titles\"]       = None\n",
    "amazon_grouped[\"Model_rec_descriptions\"] = None\n",
    "amazon_grouped[\"Model_rec_links\"]        = None\n",
    "amazon_grouped[\"Model_rec_scores\"]       = None\n",
    "\n",
    "# 4) For each unique query, run unified_query and store the top-K recs + scores\n",
    "for i, row in amazon_grouped.iterrows():\n",
    "    q = row[\"Queries\"]\n",
    "    img_url = row[\"Image_link\"][0]  # use the first image for that query\n",
    "    \n",
    "    recs, scores = unified_query(input_text=q, input_image_path=img_url, k=5)\n",
    "    \n",
    "    # Extract the fields you want from the returned DataFrame\n",
    "    amazon_grouped.at[i, \"Model_rec_titles\"]       = recs[\"product_title\"].tolist()\n",
    "    amazon_grouped.at[i, \"Model_rec_descriptions\"] = recs[\"product_description\"].tolist()\n",
    "    # If you also saved product links in your metadata, include them:\n",
    "    amazon_grouped.at[i, \"Model_rec_links\"]        = recs.get(\"product_link\", pd.Series()).tolist()\n",
    "    amazon_grouped.at[i, \"Model_rec_scores\"]       = scores.tolist()\n",
    "\n",
    "# 5) Inspect\n",
    "amazon_grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067db296-dd88-407a-acfc-e055478f9f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_grouped[\"mean_similarity@5\"] = amazon_grouped[\"Model_rec_scores\"] \\\n",
    "                                            .apply(lambda scores: np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc25fbb-3b74-4544-a1dc-1208bc141ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = amazon_grouped[\"mean_similarity@5\"].mean()\n",
    "overall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40ba7c-a0ef-42ad-9a9a-ec9aa24cc23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_grouped.to_csv(os.path.join(SAVE_DIR, \"model_recommendations.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
