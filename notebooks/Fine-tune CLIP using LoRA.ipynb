{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad27d90-9db7-4302-b192-f782aa461c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.conda/envs/test/lib/python3.13/site-packages (4.50.0)\n",
      "Requirement already satisfied: accelerate in ./.conda/envs/test/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: peft in ./.conda/envs/test/lib/python3.13/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in ./.conda/envs/test/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: torchvision in ./.conda/envs/test/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.conda/envs/test/lib/python3.13/site-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.conda/envs/test/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.conda/envs/test/lib/python3.13/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.conda/envs/test/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/test/lib/python3.13/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/test/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate peft datasets torchvision bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a15360-d5ab-4f05-88c0-e4612eafc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_meta_clean = pd.read_csv('product_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1b0643-ccdb-406d-9b47-2ebbb2ae3c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Howard LC0008 Leather Conditioner, 8-Ounce (4-...\n",
       "1         Yes to Tomatoes Detoxifying Charcoal Cleanser ...\n",
       "2          Eye Patch Black Adult with Tie Band (6 Per Pack)\n",
       "3         Tattoo Eyebrow Stickers, Waterproof Eyebrow, 4...\n",
       "4         Precision Plunger Bars for Cartridge Grips – 9...\n",
       "                                ...                        \n",
       "112548    TOPREETY 24\"120gr 3/4 Full Head clip in hair e...\n",
       "112549    Pets Playmate Pet Grooming Glove,Gentle Deshed...\n",
       "112550    [10Pack] Makeup Brushes Set Cosmetics Tools Ki...\n",
       "112551    Xcoser Pretty Party Anna Wig Hair Tails Hair S...\n",
       "112552    DVIO Men's Voyage Perfume, Spicy woody fragran...\n",
       "Name: product_title, Length: 112553, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta_clean['product_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2f1345-4af3-4e95-9008-60b44214456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_clean[\"product_text\"] = df_meta_clean.apply(lambda x : str(x[\"product_title\"]) + \" \" + str(x[\"product_description\"]), axis=1)\n",
    "\n",
    "df_train = df_meta_clean[\n",
    "    df_meta_clean[\"product_text\"].str.strip().astype(bool) &\n",
    "    df_meta_clean[\"product_image_url\"].str.strip().astype(bool)\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b59e92-8090-46f7-a323-7f3e86a60b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4022f238-e7f4-42ed-80fb-dd0083c6cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_meta_clean[[\"product_text\",\"product_image_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e64f23-5767-4f8f-b7c6-890ab8e45f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCLIPDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"product_text\"].tolist()\n",
    "        self.urls  = df[\"product_image_url\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url  = self.urls[idx]\n",
    "        try:\n",
    "            img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.new(\"RGB\",(224,224),\"white\")\n",
    "        return {\"text\": text, \"image\": img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a85e3ea0-a544-4647-98b4-6d32974d2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(text=texts,\n",
    "                    images=images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True)\n",
    "    return enc\n",
    "\n",
    "dataset = ProductCLIPDataset(df_train)\n",
    "loader  = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b675524a-2537-4616-aba4-64fbf4c742bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d95476-700e-4c6d-acdc-2c781c6fb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|███████████████████████████| 440/440 [5:53:28<00:00, 48.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 5.2189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|███████████████████████████| 440/440 [5:45:18<00:00, 47.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 4.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|███████████████████████████| 440/440 [6:10:37<00:00, 50.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 4.9190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|███████████████████████████| 440/440 [6:30:56<00:00, 53.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 4.8887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|███████████████████████████| 440/440 [6:17:37<00:00, 51.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 4.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|███████████████████████████| 440/440 [6:13:59<00:00, 51.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 4.8564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|███████████████████████████| 440/440 [6:14:45<00:00, 51.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 4.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:   0%|▏                            | 2/440 [01:33<5:37:44, 46.27s/it]"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move all to device\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # 1) Get embeddings\n",
    "        text_embs  = model.get_text_features(**{k:batch[k] for k in [\"input_ids\",\"attention_mask\"]})\n",
    "        image_embs = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "        # 2) Normalize\n",
    "        text_embs  = F.normalize(text_embs,  p=2, dim=-1)\n",
    "        image_embs = F.normalize(image_embs, p=2, dim=-1)\n",
    "\n",
    "        # 3) Similarity logits\n",
    "        logits_per_text  = text_embs @ image_embs.t()\n",
    "        logits_per_image = logits_per_text.t()\n",
    "\n",
    "        # 4) Contrastive loss\n",
    "        B = logits_per_text.size(0)\n",
    "        labels = torch.arange(B, device=device)\n",
    "        loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "        loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "        loss = (loss_t2i + loss_i2t) / 2\n",
    "\n",
    "        # 5) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184cd07-b8bb-42dc-bc2f-bdcf00c4086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"clip-lora-beauty-full\")\n",
    "processor.save_pretrained(\"clip-lora-beauty-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44380d3b-b6cd-4a2c-aef2-74fd69e113f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load base CLIP\n",
    "base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# Load your fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"clip-lora-beauty-full\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Load processor\n",
    "processor = CLIPProcessor.from_pretrained(\"clip-lora-beauty-full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14aa49-00ea-47b3-980a-a5b372462c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "text_embeddings = []\n",
    "image_embeddings = []\n",
    "\n",
    "for i, row in tqdm(df_meta_clean.iterrows(), total=len(df_meta_clean)):\n",
    "    text = row['product_text']\n",
    "    url  = row['product_image_url']\n",
    "\n",
    "    # Image loading\n",
    "    try:\n",
    "        image = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "    except:\n",
    "        image = Image.new(\"RGB\", (224, 224), \"white\")\n",
    "\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_emb = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        i_emb = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "\n",
    "    text_embeddings.append(t_emb.cpu().numpy())\n",
    "    image_embeddings.append(i_emb.cpu().numpy())\n",
    "\n",
    "text_embeddings = np.vstack(text_embeddings)\n",
    "image_embeddings = np.vstack(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b5ab5-cc12-41a6-988d-669ee12a2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"product_text_embeddings_finetuned_full.npy\", np.stack(text_embeddings))\n",
    "np.save(\"product_image_embeddings_finetuned_full.npy\", np.stack(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f647cc2-e8e6-47e5-ac0c-2db0e068497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(query_text=None, query_image_url=None, alpha=0.5, top_k=5):\n",
    "    assert query_text or query_image_url, \"Need at least text or image\"\n",
    "\n",
    "    t_emb, i_emb = None, None\n",
    "    \n",
    "    if query_text:\n",
    "        t_in = processor(text=query_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            t_emb = model.get_text_features(**t_in).cpu()  # [1, D]\n",
    "\n",
    "    if query_image_url:\n",
    "        img = Image.open(BytesIO(requests.get(query_image_url, timeout=10).content)).convert(\"RGB\")\n",
    "        i_in = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            i_emb = model.get_image_features(**i_in).cpu()# [1, D]\n",
    "\n",
    "    sims = None\n",
    "\n",
    "    if t_emb is not None and i_emb is not None:\n",
    "        sim_text = cosine_similarity(t_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(i_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_image + (1 - alpha) * sim_text\n",
    "\n",
    "    elif t_emb is not None:\n",
    "        sim_text = cosine_similarity(t_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(t_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_image + (1 - alpha) * sim_text\n",
    "\n",
    "    elif i_emb is not None:\n",
    "        sim_text = cosine_similarity(i_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(i_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_text + (1 - alpha) * sim_image\n",
    "\n",
    "    idxs = sims.argsort()[::-1][:top_k]\n",
    "    results = df_meta_clean.iloc[idxs][['product_title', 'product_image_url']].copy()\n",
    "    results['similarity_score'] = sims[idxs]\n",
    "    return results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37914b1c-a0be-423f-87bb-468f16fab29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recommend(query_text=\"photo finish Professional airbrush makeup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13345b-53ef-4d27-85b0-6663c3fa35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_url = \"https://temptupro.com/cdn/shop/products/s-one-essential-airbrush-kit-hero_2.jpg?v=1743181132&width=1780\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883485e-db04-4238-b6bf-ccb698deb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(query_image_url=query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493b19f-15e6-4015-bbe7-040d8a058d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(query_text=\" temptu airbrush makeup kit with compressor\", query_image_url= query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd564f3-f2e0-4e15-bf88-c9f9f268e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_meta_clean['product_title'].sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978d426-18f6-4041-9b20-5237c247c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
