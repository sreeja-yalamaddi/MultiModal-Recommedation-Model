{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad27d90-9db7-4302-b192-f782aa461c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.conda/envs/test/lib/python3.13/site-packages (4.50.0)\n",
      "Requirement already satisfied: accelerate in ./.conda/envs/test/lib/python3.13/site-packages (1.6.0)\n",
      "Requirement already satisfied: peft in ./.conda/envs/test/lib/python3.13/site-packages (0.15.2)\n",
      "Requirement already satisfied: datasets in ./.conda/envs/test/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: torchvision in ./.conda/envs/test/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.conda/envs/test/lib/python3.13/site-packages (0.45.5)\n",
      "Requirement already satisfied: filelock in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.conda/envs/test/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in ./.conda/envs/test/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.conda/envs/test/lib/python3.13/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in ./.conda/envs/test/lib/python3.13/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in ./.conda/envs/test/lib/python3.13/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.conda/envs/test/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.conda/envs/test/lib/python3.13/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.conda/envs/test/lib/python3.13/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/test/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/envs/test/lib/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/test/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/envs/test/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate peft datasets torchvision bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b59e92-8090-46f7-a323-7f3e86a60b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a15360-d5ab-4f05-88c0-e4612eafc14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1225847/1365325199.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"product_text\"] = df_filtered.apply(create_product_text, axis=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"product_data.csv\")\n",
    "\n",
    "# Fill missing values\n",
    "df[\"product_title\"] = df[\"product_title\"].fillna(\"\")\n",
    "df[\"product_description\"] = df[\"product_description\"].fillna(\"\")\n",
    "\n",
    "# Define keywords for filtering product titles and descriptions\n",
    "keywords = ['nail', 'shampoo', 'conditioner', 'eye', 'lip', 'ear', 'nose', 'beauty', 'cosmetic', 'hair', 'skin','hand', 'leg', 'oil' ,'makeup', 'lotion', 'cream', 'cleanser', 'moisturizer']\n",
    "\n",
    "# Filter rows where product_title or product_description contains any of the keywords\n",
    "def contains_keywords(text, keywords):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "# Apply the filter\n",
    "df_filtered = df[df[\"product_title\"].apply(lambda x: contains_keywords(x, keywords)) | \n",
    "                 df[\"product_description\"].apply(lambda x: contains_keywords(x, keywords))]\n",
    "\n",
    "# Clean and format product text\n",
    "def create_product_text(row):\n",
    "    title = row[\"product_title\"].strip()\n",
    "    description = row[\"product_description\"].strip()\n",
    "    \n",
    "    # If there's a description, include it; otherwise, just include the title\n",
    "    if description:\n",
    "        full_text = f\"Product title is: {title}\\nProduct description is: {description}\"\n",
    "    else:\n",
    "        full_text = f\"Product title is: {title}\"\n",
    "    \n",
    "    return full_text[:512]  # limit text to 512 characters (adjust length if needed)\n",
    "\n",
    "df_filtered[\"product_text\"] = df_filtered.apply(create_product_text, axis=1)\n",
    "\n",
    "# Keep only rows where product_text and product_image_url are not empty\n",
    "df_cleaned = df_filtered[\n",
    "    df_filtered[\"product_text\"].str.strip().astype(bool) & \n",
    "    df_filtered[\"product_image_url\"].str.strip().astype(bool)\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2f1345-4af3-4e95-9008-60b44214456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = df_cleaned.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e64f23-5767-4f8f-b7c6-890ab8e45f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductCLIPDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"product_text\"].tolist()\n",
    "        self.urls  = df[\"product_image_url\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url  = self.urls[idx]\n",
    "        try:\n",
    "            img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.new(\"RGB\",(224,224),\"white\")\n",
    "        return {\"text\": text, \"image\": img}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85e3ea0-a544-4647-98b4-6d32974d2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(text=texts,\n",
    "                    images=images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True)\n",
    "    return enc\n",
    "\n",
    "dataset = ProductCLIPDataset(df_train)\n",
    "loader  = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b675524a-2537-4616-aba4-64fbf4c742bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d95476-700e-4c6d-acdc-2c781c6fb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████| 157/157 [1:10:18<00:00, 26.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 4.6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████| 157/157 [1:07:04<00:00, 25.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 4.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████| 157/157 [1:03:47<00:00, 24.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 4.3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████| 157/157 [1:06:15<00:00, 25.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 4.2874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████| 157/157 [1:05:50<00:00, 25.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 4.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████| 157/157 [1:04:20<00:00, 24.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 4.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:  44%|█████████████▏                | 69/157 [28:33<35:11, 24.00s/it]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move all to device\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # 1) Get embeddings\n",
    "        text_embs  = model.get_text_features(**{k:batch[k] for k in [\"input_ids\",\"attention_mask\"]})\n",
    "        image_embs = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "        # 2) Normalize\n",
    "        text_embs  = F.normalize(text_embs,  p=2, dim=-1)\n",
    "        image_embs = F.normalize(image_embs, p=2, dim=-1)\n",
    "\n",
    "        # 3) Similarity logits\n",
    "        logits_per_text  = text_embs @ image_embs.t()\n",
    "        logits_per_image = logits_per_text.t()\n",
    "\n",
    "        # 4) Contrastive loss\n",
    "        B = logits_per_text.size(0)\n",
    "        labels = torch.arange(B, device=device)\n",
    "        loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "        loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "        loss = (loss_t2i + loss_i2t) / 2\n",
    "\n",
    "        # 5) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg:.4f}\")\n",
    "print(time.time() -  start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2184cd07-b8bb-42dc-bc2f-bdcf00c4086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"clip-lora-beauty-20k\")\n",
    "processor.save_pretrained(\"clip-lora-beauty-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a6da8-db82-44cd-b819-683b575c3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44380d3b-b6cd-4a2c-aef2-74fd69e113f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load base CLIP\n",
    "base_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# Load your fine-tuned adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"clip-lora-beauty-full\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Load processor\n",
    "processor = CLIPProcessor.from_pretrained(\"clip-lora-beauty-full\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14aa49-00ea-47b3-980a-a5b372462c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "text_embeddings = []\n",
    "image_embeddings = []\n",
    "\n",
    "for i, row in tqdm(df_meta_clean.iterrows(), total=len(df_meta_clean)):\n",
    "    text = row['product_text']\n",
    "    url  = row['product_image_url']\n",
    "\n",
    "    # Image loading\n",
    "    try:\n",
    "        image = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "    except:\n",
    "        image = Image.new(\"RGB\", (224, 224), \"white\")\n",
    "\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_emb = model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "        i_emb = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "\n",
    "    text_embeddings.append(t_emb.cpu().numpy())\n",
    "    image_embeddings.append(i_emb.cpu().numpy())\n",
    "\n",
    "text_embeddings = np.vstack(text_embeddings)\n",
    "image_embeddings = np.vstack(image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020b5ab5-cc12-41a6-988d-669ee12a2417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(\"product_text_embeddings_finetuned_full.npy\", np.stack(text_embeddings))\n",
    "np.save(\"product_image_embeddings_finetuned_full.npy\", np.stack(image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f647cc2-e8e6-47e5-ac0c-2db0e068497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(query_text=None, query_image_url=None, alpha=0.5, top_k=5):\n",
    "    assert query_text or query_image_url, \"Need at least text or image\"\n",
    "\n",
    "    t_emb, i_emb = None, None\n",
    "    \n",
    "    if query_text:\n",
    "        t_in = processor(text=query_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            t_emb = model.get_text_features(**t_in).cpu()  # [1, D]\n",
    "\n",
    "    if query_image_url:\n",
    "        img = Image.open(BytesIO(requests.get(query_image_url, timeout=10).content)).convert(\"RGB\")\n",
    "        i_in = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            i_emb = model.get_image_features(**i_in).cpu()# [1, D]\n",
    "\n",
    "    sims = None\n",
    "\n",
    "    if t_emb is not None and i_emb is not None:\n",
    "        sim_text = cosine_similarity(t_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(i_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_image + (1 - alpha) * sim_text\n",
    "\n",
    "    elif t_emb is not None:\n",
    "        sim_text = cosine_similarity(t_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(t_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_image + (1 - alpha) * sim_text\n",
    "\n",
    "    elif i_emb is not None:\n",
    "        sim_text = cosine_similarity(i_emb, text_embeddings)[0]\n",
    "        sim_image = cosine_similarity(i_emb, image_embeddings)[0]\n",
    "        sims = alpha * sim_text + (1 - alpha) * sim_image\n",
    "\n",
    "    idxs = sims.argsort()[::-1][:top_k]\n",
    "    results = df_meta_clean.iloc[idxs][['product_title', 'product_image_url']].copy()\n",
    "    results['similarity_score'] = sims[idxs]\n",
    "    return results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37914b1c-a0be-423f-87bb-468f16fab29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "recommend(query_text=\"photo finish Professional airbrush makeup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13345b-53ef-4d27-85b0-6663c3fa35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_url = \"https://temptupro.com/cdn/shop/products/s-one-essential-airbrush-kit-hero_2.jpg?v=1743181132&width=1780\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8883485e-db04-4238-b6bf-ccb698deb4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(query_image_url=query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493b19f-15e6-4015-bbe7-040d8a058d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(query_text=\" temptu airbrush makeup kit with compressor\", query_image_url= query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd564f3-f2e0-4e15-bf88-c9f9f268e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_meta_clean['product_title'].sample(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978d426-18f6-4041-9b20-5237c247c93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
