{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7a7ccb-fc66-4b29-8fe7-f1737b75b8d5",
   "metadata": {},
   "source": [
    "!pip install transformers accelerate peft datasets torchvision bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c49b19d-e36d-4bdd-b305-58a40cbe5e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "from utils import load_and_clean_data, get_model, save_model_and_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b48ef4-1b17-4346-b247-e44c1b412115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "CSV_PATH      = \"product_data.csv\"\n",
    "SAVE_DIR      = \"artifacts_lora\"\n",
    "BATCH_SIZE    = 128\n",
    "NUM_EPOCHS    = 10\n",
    "LR            = 2e-5\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME    = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "os.makedirs(SAVE_DIR,     exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6e64f23-5767-4f8f-b7c6-890ab8e45f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── DATASET +  collate_fn ────────────────────────────────────────────────\n",
    "class ProductCLIPDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts     = df[\"product_text\"].tolist()\n",
    "        self.urls      = df[\"product_image_url\"].tolist()\n",
    "        proc = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "        self.tokenizer       = proc.tokenizer\n",
    "        self.image_processor = proc.image_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url  = self.urls[idx]\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=5)\n",
    "            img  = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        except:\n",
    "            img  = Image.new(\"RGB\", (224,224), \"white\")\n",
    "        return {\"text\": text, \"image\": img}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts  = [ex[\"text\"]  for ex in batch]\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "\n",
    "        tok = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        imgs = self.image_processor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      tok.input_ids,\n",
    "            \"attention_mask\": tok.attention_mask,\n",
    "            \"pixel_values\":   imgs\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1fa4bd6-dade-4509-b149-2d675765a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_and_clean_data(CSV_PATH)\n",
    "df_train = df.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a85e3ea0-a544-4647-98b4-6d32974d2d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(text=texts,\n",
    "                    images=images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True)\n",
    "    return enc\n",
    "\n",
    "dataset = ProductCLIPDataset(df_train)\n",
    "loader  = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b675524a-2537-4616-aba4-64fbf4c742bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(approach=\"lora\", save_dir=None)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d95476-700e-4c6d-acdc-2c781c6fb963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|███████████████████████████| 157/157 [1:10:18<00:00, 26.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 4.6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████| 157/157 [1:07:04<00:00, 25.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 4.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████| 157/157 [1:03:47<00:00, 24.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 4.3517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████| 157/157 [1:06:15<00:00, 25.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 4.2874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████| 157/157 [1:05:50<00:00, 25.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 4.2533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████| 157/157 [1:04:20<00:00, 24.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 4.2296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████| 157/157 [1:04:18<00:00, 24.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 4.2136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████| 157/157 [1:05:54<00:00, 25.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg loss: 4.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|███████████████████████████| 157/157 [1:03:54<00:00, 24.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 avg loss: 4.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████████████████████| 157/157 [1:03:50<00:00, 24.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 avg loss: 4.1785\n",
      "39334.03454661369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Move all to device\n",
    "        batch = {k:v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # 1) Get embeddings\n",
    "        text_embs  = model.get_text_features(**{k:batch[k] for k in [\"input_ids\",\"attention_mask\"]})\n",
    "        image_embs = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "        # 2) Normalize\n",
    "        text_embs  = F.normalize(text_embs,  p=2, dim=-1)\n",
    "        image_embs = F.normalize(image_embs, p=2, dim=-1)\n",
    "\n",
    "        # 3) Similarity logits\n",
    "        logits_per_text  = text_embs @ image_embs.t()\n",
    "        logits_per_image = logits_per_text.t()\n",
    "\n",
    "        # 4) Contrastive loss\n",
    "        B = logits_per_text.size(0)\n",
    "        labels = torch.arange(B, device=device)\n",
    "        loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "        loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "        loss = (loss_t2i + loss_i2t) / 2\n",
    "\n",
    "        # 5) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg = total_loss / len(loader)\n",
    "    print(f\"Epoch {epoch+1} avg loss: {avg:.4f}\")\n",
    "print(time.time() -  start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a43930b1-19b3-441f-91bf-861a4b96501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time taken : 655.57\n"
     ]
    }
   ],
   "source": [
    "print(\"Training time taken :\" , round(39334.03454661369/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269f2c8a-5bcc-41a4-9fbd-d6d680f97aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_and_processor(model, SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4852a979-ef2e-44fb-aea4-241d3e349849",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "625caccd-b6fb-452c-a725-fce2e6c0e5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad5162-2a7e-4077-832f-c54c8b6a3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Load Model & Generate Embeddings ────────────────────────────────────\n",
    "tuned_model = get_model(approach=\"lora\", save_dir=SAVE_DIR)\n",
    "tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65c449c0-b2f3-46c0-a4ca-397ab47458a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts  = [ex[\"text\"]  for ex in batch]\n",
    "    images = [ex[\"image\"] for ex in batch]\n",
    "    enc = processor(\n",
    "        text=texts,\n",
    "        images=images,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # **All tensors here are on CPU**—no .to(device)!\n",
    "    return {\n",
    "        \"input_ids\":      enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"pixel_values\":   enc[\"pixel_values\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9dbd1769-cf56-4731-8871-b35b0dcc92a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataset, batch_size=32, num_workers=4):\n",
    "    \"\"\"\n",
    "    Returns two CPU tensors: text_embs [N, D], image_embs [N, D].\n",
    "    \"\"\"\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,            # collate_fn returns CPU\n",
    "        collate_fn=dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    model.eval().to(DEVICE)\n",
    "    all_text, all_image = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Generating embeddings\"):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "            # 4️Get features\n",
    "            t = model.get_text_features(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            i = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "\n",
    "            # 5️Normalize & collect on CPU\n",
    "            all_text.append(F.normalize(t,  dim=-1).cpu())\n",
    "            all_image.append(F.normalize(i, dim=-1).cpu())\n",
    "\n",
    "    text_embs  = torch.cat(all_text,  dim=0)\n",
    "    image_embs = torch.cat(all_image, dim=0)\n",
    "    return text_embs, image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49f763f1-4586-45fe-b63e-d830e238a58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|                           | 0/1377 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating embeddings: 100%|██████████████| 1377/1377 [1:12:03<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset = ProductCLIPDataset(df)\n",
    "text_embs, image_embs = generate_embeddings(model, dataset, batch_size=64, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4854ec4d-72c2-42d6-8a69-69f0c894e3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shapes: torch.Size([88083, 512]) torch.Size([88083, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Embeddings shapes:\", text_embs.shape, image_embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ebe32696-7259-4dcd-a5c8-2eeee9f05b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to: artifacts_lora/faiss.index\n"
     ]
    }
   ],
   "source": [
    "save_embeddings(text_embs, image_embs, SAVE_DIR)\n",
    "\n",
    "combined = F.normalize((text_embs + image_embs) / 2, dim=-1)\n",
    "index_path = build_faiss_index(combined, SAVE_DIR)\n",
    "print(\"FAISS index saved to:\", index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4962c0fe-4af0-4065-96bb-6a2adcef81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_query(\n",
    "    input_text: str = None,\n",
    "    input_image_path: str = None,\n",
    "    approach: str = \"zero_shot\",\n",
    "    save_dir: str = None,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Query the FAISS index for a given approach (zero_shot, lora, or lora_opt).\n",
    "    \"\"\"\n",
    "    \n",
    "    model = get_model(approach=approach, save_dir=save_dir)\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    df  = pd.read_csv(\"product_data.csv\")\n",
    "    idx = load_faiss_index(save_dir)\n",
    "\n",
    "    # 2) Load processor, then split tokenizer and image_processor\n",
    "    proc = CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    tokenizer       = proc.tokenizer\n",
    "    image_processor = proc.image_processor\n",
    "\n",
    "    # 3) Prepare inputs\n",
    "    # We batch as a single‐element batch so outputs remain [1, D]\n",
    "    batch = {}\n",
    "    if input_text:\n",
    "        tok = tokenizer(\n",
    "            [input_text],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"input_ids\"]      = tok.input_ids.to(DEVICE)\n",
    "        batch[\"attention_mask\"] = tok.attention_mask.to(DEVICE)\n",
    "\n",
    "    if input_image_path:\n",
    "        if input_image_path.startswith(\"http\"):\n",
    "            resp = requests.get(input_image_path, timeout=5)\n",
    "            img  = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            img  = Image.open(input_image_path).convert(\"RGB\")\n",
    "\n",
    "        img_out = image_processor(\n",
    "            images=[img],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        batch[\"pixel_values\"] = img_out.pixel_values.to(DEVICE)\n",
    "\n",
    "    # 4) Forward pass\n",
    "    with torch.no_grad():\n",
    "        if \"input_ids\" in batch and \"pixel_values\" in batch:\n",
    "            t_emb = model.get_text_features(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "            i_emb = model.get_image_features(\n",
    "                pixel_values=batch[\"pixel_values\"]\n",
    "            )\n",
    "            q_emb = (t_emb + i_emb) / 2\n",
    "        elif \"input_ids\" in batch:\n",
    "            q_emb = model.get_text_features(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"]\n",
    "            )\n",
    "        else:\n",
    "            q_emb = model.get_image_features(\n",
    "                pixel_values=batch[\"pixel_values\"]\n",
    "            )\n",
    "\n",
    "    # 5) Normalize, search FAISS\n",
    "    q_norm = F.normalize(q_emb, dim=-1).cpu().numpy().astype(\"float32\")\n",
    "    scores, ids = idx.search(q_norm, k)\n",
    "\n",
    "    top_df     = df.iloc[ids[0]].reset_index(drop=True)\n",
    "    top_scores = scores[0]\n",
    "    return top_df, top_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea326433-6f26-476c-ae60-898693445da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-only recommendations:\n",
      "                                       product_title  \\\n",
      "0  Svanslashes Eyelash Extensions D Curl - Premiu...   \n",
      "1  Silicone Lotion Bottles Squeezable Leak Proof ...   \n",
      "2      Uppercut Deluxe Men's Conditioner - Pack of 3   \n",
      "3  NOKMOPO Women Fashion Keep Warm Knitting Headb...   \n",
      "4  613 Blonde Deep Wave 3 Bundles 100% Brazilian ...   \n",
      "\n",
      "                                   product_image_url  \n",
      "0  https://m.media-amazon.com/images/I/81TEjbcXTj...  \n",
      "1  https://m.media-amazon.com/images/I/41+SopylN9...  \n",
      "2  https://m.media-amazon.com/images/I/61YDP21AdP...  \n",
      "3  https://m.media-amazon.com/images/I/61-zjZBMXL...  \n",
      "4  https://m.media-amazon.com/images/I/71ZGWHAjlR...   [0.5009656  0.50083387 0.4996715  0.49935097 0.4992208 ]\n",
      "Image-only recommendations:\n",
      "                                       product_title  \\\n",
      "0  Detangling Brush Pink - Detangle Brush - No Ta...   \n",
      "1  RORASA Magnetic Eyelashes with Magnetic Eyelin...   \n",
      "2  L.O.L Surprise! Townley Girl Jumbo Hair Access...   \n",
      "3  Wiggly Dog and Cat Brush Double Sided - 8.85 i...   \n",
      "4  Freeman Facial Charcoal+Sea Salt Detoxify Shee...   \n",
      "\n",
      "                                   product_image_url  \n",
      "0  https://m.media-amazon.com/images/I/61NJP-N-Bc...  \n",
      "1  https://m.media-amazon.com/images/I/61I2oQJvqv...  \n",
      "2  https://m.media-amazon.com/images/I/9159REM96U...  \n",
      "3  https://m.media-amazon.com/images/I/71J2URDaxY...  \n",
      "4  https://m.media-amazon.com/images/I/816DTQAgtF...   [0.9095386  0.9052327  0.90374786 0.9037478  0.90172017]\n",
      "Text+Image recommendations:\n",
      "                                       product_title  \\\n",
      "0  Abody 32Pcs Professional Make Up Brush Set Cos...   \n",
      "1  VNDEFUL 5Pcs Colorful Oval Sponge Facial Washi...   \n",
      "2  1 Pack Red + 10 Pcs Replacement Cotton Reusabl...   \n",
      "3  28 Pcs Donut Hair Bun Maker Set Include 4 Pcs ...   \n",
      "4  L.O.L Surprise! Townley Girl Jumbo Hair Access...   \n",
      "\n",
      "                                   product_image_url  \n",
      "0  https://m.media-amazon.com/images/I/51t5J-Pnv6...  \n",
      "1  https://m.media-amazon.com/images/I/41Kn95B5xZ...  \n",
      "2  https://m.media-amazon.com/images/I/51pVMOLZvH...  \n",
      "3  https://m.media-amazon.com/images/I/81-7MorN0l...  \n",
      "4  https://m.media-amazon.com/images/I/9159REM96U...   [0.73147464 0.7290896  0.72776526 0.72535354 0.7245885 ]\n"
     ]
    }
   ],
   "source": [
    "## SAMPLE TESTING\n",
    "\n",
    "q_text = \"photo finish Professional airbrush makeup\"\n",
    "q_img  = \"https://temptupro.com/cdn/shop/products/s-one-essential-airbrush-kit-hero_2.jpg?v=1743181132&width=1780\"\n",
    "\n",
    "recs_text, scores_text = unified_query(\n",
    "    input_text=q_text,\n",
    "    input_image_path=None,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Text-only recommendations:\")\n",
    "print(recs_text[[\"product_title\",\"product_image_url\"]], scores_text)\n",
    "\n",
    "recs_img, scores_img = unified_query(\n",
    "    input_text=None,\n",
    "    input_image_path=q_img,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Image-only recommendations:\")\n",
    "print(recs_img[[\"product_title\",\"product_image_url\"]], scores_img)\n",
    "\n",
    "recs_both, scores_both = unified_query(\n",
    "    input_text=q_text,\n",
    "    input_image_path=q_img,\n",
    "    save_dir=SAVE_DIR,\n",
    "    k=5\n",
    ")\n",
    "print(\"Text+Image recommendations:\")\n",
    "print(recs_both[[\"product_title\",\"product_image_url\"]], scores_both)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
