{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c6534d3-e461-4b19-9dd5-00ec9c02f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "\n",
    "!pip install transformers datasets peft accelerate faiss-cpu --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "import requests\n",
    "from io import BytesIO\n",
    "scaler = GradScaler()\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445252b6-8b0b-4189-8381-aa4f3b24b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "DATA_PATH = \"./product_data.csv\"\n",
    "N_SAMPLES = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611b45e-c16b-4998-b8c3-87219021e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Fill missing values\n",
    "df[\"product_title\"] = df[\"product_title\"].fillna(\"\")\n",
    "df[\"product_description\"] = df[\"product_description\"].fillna(\"\")\n",
    "\n",
    "# Define keywords for filtering product titles and descriptions\n",
    "keywords = ['nail', 'shampoo', 'conditioner', 'eye', 'lip', 'ear', 'nose', 'beauty', 'cosmetic', 'hair', 'skin','hand', 'leg', 'oil' ,'makeup', 'lotion', 'cream', 'cleanser', 'moisturizer']\n",
    "\n",
    "# Filter rows where product_title or product_description contains any of the keywords\n",
    "def contains_keywords(text, keywords):\n",
    "    return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "\n",
    "# Apply the filter\n",
    "df_filtered = df[df[\"product_title\"].apply(lambda x: contains_keywords(x, keywords)) | \n",
    "                 df[\"product_description\"].apply(lambda x: contains_keywords(x, keywords))]\n",
    "\n",
    "# Clean and format product text\n",
    "def create_product_text(row):\n",
    "    title = row[\"product_title\"].strip()\n",
    "    description = row[\"product_description\"].strip()\n",
    "    \n",
    "    # If there's a description, include it; otherwise, just include the title\n",
    "    if description:\n",
    "        full_text = f\"Product title is: {title}\\nProduct description is: {description}\"\n",
    "    else:\n",
    "        full_text = f\"Product title is: {title}\"\n",
    "    \n",
    "    return full_text[:512]  # limit text to 512 characters (adjust length if needed)\n",
    "\n",
    "df_filtered[\"product_text\"] = df_filtered.apply(create_product_text, axis=1)\n",
    "\n",
    "# Keep only rows where product_text and product_image_url are not empty\n",
    "df_cleaned = df_filtered[\n",
    "    df_filtered[\"product_text\"].str.strip().astype(bool) & \n",
    "    df_filtered[\"product_image_url\"].str.strip().astype(bool)\n",
    "].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f1505-917b-48f3-b3db-f1bd50bae51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['product_text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b564be2f-c39e-4db5-bfd9-bbcf7b506395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, dataframe, model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        self.texts = dataframe[\"product_text\"].tolist()\n",
    "        self.image_urls = dataframe[\"product_image_url\"].tolist()\n",
    "        # Load a single fast processor\n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url  = self.image_urls[idx]\n",
    "        try:\n",
    "            img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "        except:\n",
    "            img = Image.new(\"RGB\", (224,224), \"white\")\n",
    "        return {\"text\": text, \"image\": img}\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        texts  = [ex[\"text\"] for ex in batch]\n",
    "        images = [ex[\"image\"] for ex in batch]\n",
    "\n",
    "        # 1) Tokenize text\n",
    "        tokenized = self.processor.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 2) Preprocess images\n",
    "        # Note: depending on your transformers version this may be `.feature_extractor` or `.image_processor`\n",
    "        image_inputs = self.processor.image_processor(\n",
    "            images=images,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # 3) Merge\n",
    "        tokenized[\"pixel_values\"] = image_inputs[\"pixel_values\"]\n",
    "        return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d4a17-5e52-462a-bf84-1135e2eed460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL + LORA\n",
    "def get_model_with_lora():\n",
    "    base = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    model = get_peft_model(base, config)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f0d27-96d8-4dc9-9d0e-47ef0e028eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    scaler = GradScaler()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            with autocast(device_type='cuda'):  # <<== Fix here\n",
    "                text_embs = model.get_text_features(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"]\n",
    "                )\n",
    "                image_embs = model.get_image_features(\n",
    "                    pixel_values=batch[\"pixel_values\"]\n",
    "                )\n",
    "                # 3) Similarity logits\n",
    "                logits_per_text  = text_embs @ image_embs.t()\n",
    "                logits_per_image = logits_per_text.t()\n",
    "        \n",
    "                # 4) Contrastive loss\n",
    "                B = logits_per_text.size(0)\n",
    "                labels = torch.arange(B, device=device)\n",
    "                loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "                loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "                loss = (loss_t2i + loss_i2t) / 2\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(dataloader):.4f}\")\n",
    "    total_training_time = time.time() - start_time  # Total training time\n",
    "    print(f\"Total Training Time: {total_training_time / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f19566-fd35-4382-b973-1d0d8e16da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataset):\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,  \n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        collate_fn=dataset.collate_fn  \n",
    "    )\n",
    "    text_embs, image_embs = [], []  \n",
    "    model.eval()  \n",
    "    model.to(device)  \n",
    "    with torch.no_grad():  \n",
    "        for batch in tqdm(dataloader, desc=\"Generating embeddings\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            text_embeddings = model.get_text_features(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            image_embeddings = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "            text_embs.append(F.normalize(text_embeddings, p=2, dim=-1).cpu())  # L2 normalization\n",
    "            image_embs.append(F.normalize(image_embeddings, p=2, dim=-1).cpu())  # L2 normalization\n",
    "    text_embs = torch.cat(text_embs, dim=0)\n",
    "    image_embs = torch.cat(image_embs, dim=0)\n",
    "    return text_embs, image_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe21a2-86b0-4073-b126-a8f272a893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FAISS INDEXING\n",
    "def build_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings.numpy())\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adaf8e2-d5e2-418b-9b92-ce862711d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL FINETUNING \n",
    "df_train = df_cleaned.sample(N_SAMPLES)\n",
    "dataset = ProductDataset(df_train, model_name=MODEL_NAME)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=dataset.collate_fn\n",
    ")\n",
    "\n",
    "model = get_model_with_lora()\n",
    "train_model(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4b8a1-c0cd-40ce-8c88-d224612e9840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings after fine-tuning and building faiss index\n",
    "dataset = ProductDataset(df_cleaned, model_name=MODEL_NAME)\n",
    "text_embs, image_embs = generate_embeddings(model, dataset)\n",
    "combined_embs = F.normalize(text_embs + image_embs, dim=-1)  # [N, D]\n",
    "index = build_faiss_index(combined_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542bdd7-a7b7-4658-8327-8ddbc22d0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_query(input_text=None, input_image_path=None, k=5):\n",
    "    assert input_text or input_image_path, \"Provide at least text or image input\"\n",
    "\n",
    "    inputs = {}\n",
    "    if input_text:\n",
    "        inputs.update({\"text\": input_text})\n",
    "    if input_image_path:\n",
    "        if input_image_path.startswith(\"http\"):\n",
    "            response = requests.get(input_image_path)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        inputs.update({\"images\": image})\n",
    "\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    encoded = processor(return_tensors=\"pt\", padding=True, truncation=True, **inputs)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if input_text and input_image_path:\n",
    "            text_emb = model.get_text_features(input_ids=encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            image_emb = model.get_image_features(pixel_values=encoded[\"pixel_values\"])\n",
    "            query_emb =(text_emb + image_emb) / 2\n",
    "        elif input_text:\n",
    "            text_emb = model.get_text_features(input_ids=encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            query_emb = text_emb\n",
    "        else:\n",
    "            image_emb = model.get_image_features(pixel_values=encoded[\"pixel_values\"])\n",
    "            query_emb = image_emb\n",
    "\n",
    "    # FAISS expects NumPy array in float32\n",
    "    query_emb = F.normalize(query_emb, dim=-1)\n",
    "    query_np = query_emb.cpu().numpy().astype(\"float32\")\n",
    "    faiss_index = faiss.read_index(os.path.join(SAVE_DIR, \"faiss_index_mp.index\"))\n",
    "    # Perform the search\n",
    "    scores, indices = faiss_index.search(query_np, k)\n",
    "    top_items = df.iloc[indices[0]]\n",
    "    top_scores = scores[0]\n",
    "\n",
    "    return top_items, top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2812d-df22-4538-97ee-02f18565d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Define the save directory\n",
    "SAVE_DIR = \"artifacts_fp_20k_clip\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(os.path.join(SAVE_DIR, \"clip_lora_model_mp\"))\n",
    "\n",
    "CLIPProcessor.from_pretrained(MODEL_NAME, use_fast=True).save_pretrained(os.path.join(SAVE_DIR, \"clip_processor_mp\"))\n",
    "\n",
    "torch.save(text_embs, os.path.join(SAVE_DIR, \"text_embeddings_mp.pt\"))\n",
    "torch.save(image_embs, os.path.join(SAVE_DIR, \"image_embeddings_mp.pt\"))\n",
    "torch.save(combined_embs, os.path.join(SAVE_DIR, \"combined_embeddings_mp.pt\"))\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"product_metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(df_train.to_dict(), f)\n",
    "\n",
    "faiss.write_index(index, os.path.join(SAVE_DIR, \"faiss_index_mp.index\"))\n",
    "\n",
    "print(f\"Model, processor, embeddings, metadata, and FAISS index saved in {SAVE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6c5ef9-cf60-49eb-9703-45fa2a220862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_embs  = normalize(torch.load(os.path.join(SAVE_DIR, \"text_embeddings_mp.pt\")).to(device))\n",
    "image_embs = normalize(torch.load(os.path.join(SAVE_DIR, \"image_embeddings_mp.pt\")).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94382adf-996b-4ab0-93b1-1b0c248bb6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_query(input_text=\"photo finish Professional airbrush makeup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d59e3-323c-4cab-8114-f641c1a082b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_url = \"https://temptupro.com/cdn/shop/products/s-one-essential-airbrush-kit-hero_2.jpg?v=1743181132&width=1780\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f368eb0-e385-4c33-9cb3-03beec26d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_query(input_image_path= query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce93405c-b833-48ed-aa58-e84e91763e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unified_query(input_text=\"airbrush makeup kit with compressor\", input_image_path= query_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d19ecf2-6ca5-4c14-ae3b-35dd3df8fa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load your queries\n",
    "df_queries = pd.read_excel(\"Amazon_recom_queries.xlsx\")\n",
    "\n",
    "# 2) Group by unique Queries, collecting Amazon’s ground-truth lists\n",
    "amazon_grouped = df_queries.groupby(\"Queries\").agg({\n",
    "    \"Product_title\":       list,\n",
    "    \"Product_description\": list,\n",
    "    \"Product_link\":        list,\n",
    "    \"Image_link\":          list\n",
    "}).reset_index()\n",
    "\n",
    "# 3) Prepare columns to hold your model’s recommendations\n",
    "amazon_grouped[\"Model_rec_titles\"]       = None\n",
    "amazon_grouped[\"Model_rec_descriptions\"] = None\n",
    "amazon_grouped[\"Model_rec_links\"]        = None\n",
    "amazon_grouped[\"Model_rec_scores\"]       = None\n",
    "\n",
    "# 4) For each unique query, run unified_query and store the top-K recs + scores\n",
    "for i, row in amazon_grouped.iterrows():\n",
    "    q = row[\"Queries\"]\n",
    "    img_url = row[\"Image_link\"][0]  # use the first image for that query\n",
    "    \n",
    "    recs, scores = unified_query(input_text=q, input_image_path=img_url, k=5)\n",
    "    \n",
    "    # Extract the fields you want from the returned DataFrame\n",
    "    amazon_grouped.at[i, \"Model_rec_titles\"]       = recs[\"product_title\"].tolist()\n",
    "    amazon_grouped.at[i, \"Model_rec_descriptions\"] = recs[\"product_description\"].tolist()\n",
    "    # If you also saved product links in your metadata, include them:\n",
    "    amazon_grouped.at[i, \"Model_rec_links\"]        = recs.get(\"product_link\", pd.Series()).tolist()\n",
    "    amazon_grouped.at[i, \"Model_rec_scores\"]       = scores.tolist()\n",
    "\n",
    "# 5) Inspect\n",
    "amazon_grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d567e9a3-b146-4257-a675-e0953c1b52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_grouped[\"mean_similarity@5\"] = amazon_grouped[\"Model_rec_scores\"] \\\n",
    "                                            .apply(lambda scores: np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8943cfb8-8d80-4a32-9c6b-12313f48b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = amazon_grouped[\"mean_similarity@5\"].mean()\n",
    "overall_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8673aa-d456-4a50-900d-72efa59d904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_grouped.to_csv(os.path.join(SAVE_DIR, \"model_recommendations.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974dbf48-740f-4e74-8f47-db80a8ec8232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
