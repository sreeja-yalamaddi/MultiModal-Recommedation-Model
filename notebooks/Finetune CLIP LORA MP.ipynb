{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6534d3-e461-4b19-9dd5-00ec9c02f76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_963966/3143232745.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "\n",
    "!pip install transformers datasets peft accelerate faiss-cpu --quiet\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445252b6-8b0b-4189-8381-aa4f3b24b4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "DATA_PATH = \"./product_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b564be2f-c39e-4db5-bfd9-bbcf7b506395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "class ProductDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.texts = dataframe[\"product_text\"].tolist()\n",
    "        self.image_urls = dataframe[\"product_image_url\"].tolist()\n",
    "        self.processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        url = self.image_urls[idx]\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        except Exception:\n",
    "            image = Image.new(\"RGB\", (224, 224), \"white\")  # fallback blank image\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"image\": image\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "    texts  = [x[\"text\"] for x in batch]\n",
    "    images = [x[\"image\"] for x in batch]\n",
    "    enc = processor(text=texts, images=images, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344d4a17-5e52-462a-bf84-1135e2eed460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL + LORA\n",
    "def get_model_with_lora():\n",
    "    base = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "    config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    model = get_peft_model(base, config)\n",
    "    return model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b1f0d27-96d8-4dc9-9d0e-47ef0e028eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "def train_model(model, dataloader):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "    \n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "            with autocast():  # <<<<<<< Mixed Precision starts here\n",
    "                text_embs = model.get_text_features(\n",
    "                    input_ids=batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"]\n",
    "                )\n",
    "                image_embs = model.get_image_features(\n",
    "                    pixel_values=batch[\"pixel_values\"]\n",
    "                )\n",
    "    \n",
    "                text_embs = F.normalize(text_embs, p=2, dim=-1)\n",
    "                image_embs = F.normalize(image_embs, p=2, dim=-1)\n",
    "    \n",
    "                logits_per_text = text_embs @ image_embs.t()\n",
    "                logits_per_image = logits_per_text.t()\n",
    "\n",
    "                B = logits_per_text.size(0)\n",
    "                labels = torch.arange(B, device=device)\n",
    "    \n",
    "                loss_t2i = F.cross_entropy(logits_per_text, labels)\n",
    "                loss_i2t = F.cross_entropy(logits_per_image, labels)\n",
    "                loss = (loss_t2i + loss_i2t) / 2\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()        # <<< Scaled backprop\n",
    "                scaler.step(optimizer)               # <<< Scaled optimizer step\n",
    "                scaler.update()                      # <<< Update the scaler\n",
    "    \n",
    "    \n",
    "                total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch+1} avg loss: {total_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f19566-fd35-4382-b973-1d0d8e16da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING GENERATION\n",
    "def generate_embeddings(model, dataset):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    text_embs, image_embs = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            t = model.get_text_features(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "            i = model.get_image_features(pixel_values=batch[\"pixel_values\"])\n",
    "            text_embs.append(F.normalize(t, dim=-1).cpu())\n",
    "            image_embs.append(F.normalize(i, dim=-1).cpu())\n",
    "\n",
    "    return torch.cat(text_embs), torch.cat(image_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fe21a2-86b0-4073-b126-a8f272a893c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  FAISS INDEXING\n",
    "def build_faiss_index(embeddings):\n",
    "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "    index.add(embeddings.numpy())\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0743682b-e91f-4674-b3e1-c486c026fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOMMENDATION\n",
    "def recommend(query_emb, index, df, k=5):\n",
    "    query_emb = F.normalize(query_emb, dim=-1).cpu().numpy()\n",
    "    scores, ids = index.search(query_emb, k)\n",
    "    return df.iloc[ids[0]], scores[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0c38e-9517-45fa-9d28-82e227badfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Epoch 1/10:   0%|                                       | 0/880 [00:00<?, ?it/s]/tmp/ipykernel_963966/3637919624.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # <<<<<<< Mixed Precision starts here\n",
      "Epoch 1/10: 100%|███████████████████████████| 880/880 [6:32:43<00:00, 26.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss: 4.4192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|███████████████████████████| 880/880 [6:20:23<00:00, 25.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg loss: 4.2260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|███████████████████████████| 880/880 [6:14:21<00:00, 25.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg loss: 4.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|███████████████████████████| 880/880 [6:19:14<00:00, 25.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg loss: 4.1649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|███████████████████████████| 880/880 [6:27:05<00:00, 26.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg loss: 4.1514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|███████████████████████████| 880/880 [6:08:50<00:00, 25.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 avg loss: 4.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|███████████████████████████| 880/880 [6:21:39<00:00, 26.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 avg loss: 4.1335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|███████████████████████████| 880/880 [6:27:49<00:00, 26.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 avg loss: 4.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:  57%|██████████████▎          | 505/880 [3:48:39<2:45:37, 26.50s/it]"
     ]
    }
   ],
   "source": [
    "# RUN ALL\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df[\"product_text\"] = df.apply(lambda x : str(x[\"product_title\"]) + \" \" + str(x[\"product_description\"]), axis=1)\n",
    "\n",
    "df_train = df[\n",
    "    df[\"product_text\"].str.strip().astype(bool) &\n",
    "    df[\"product_image_url\"].str.strip().astype(bool)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "dataset = ProductDataset(df_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model_with_lora()\n",
    "train_model(model, dataloader)\n",
    "\n",
    "# Generate embeddings after fine-tuning\n",
    "text_embs, image_embs = generate_embeddings(model, dataset)\n",
    "combined_embs = (text_embs + image_embs) / 2\n",
    "\n",
    "# Build FAISS\n",
    "index = build_faiss_index(combined_embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b542bdd7-a7b7-4658-8327-8ddbc22d0c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Setup preprocessing\n",
    "image_preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Prepare model for inference\n",
    "model.eval()\n",
    "\n",
    "def normalize(tensor):\n",
    "    return F.normalize(tensor, dim=-1)\n",
    "\n",
    "# Unified Query Function\n",
    "def unified_query(input_text=None, input_image_path=None, k=5):\n",
    "    assert input_text or input_image_path, \"Provide at least text or image input\"\n",
    "\n",
    "    inputs = {}\n",
    "    if input_text:\n",
    "        inputs.update({\"text\": input_text})\n",
    "    if input_image_path:\n",
    "        image = Image.open(input_image_path).convert(\"RGB\")\n",
    "        inputs.update({\"images\": image})\n",
    "\n",
    "    processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "    encoded = processor(return_tensors=\"pt\", padding=True, truncation=True, **inputs)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if input_text and input_image_path:\n",
    "            text_emb  = model.get_text_features(input_ids=encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            image_emb = model.get_image_features(pixel_values=encoded[\"pixel_values\"])\n",
    "            query_emb = normalize((text_emb + image_emb) / 2)\n",
    "        elif input_text:\n",
    "            text_emb  = model.get_text_features(input_ids=encoded[\"input_ids\"], attention_mask=encoded[\"attention_mask\"])\n",
    "            query_emb = normalize(text_emb)\n",
    "        else:\n",
    "            image_emb = model.get_image_features(pixel_values=encoded[\"pixel_values\"])\n",
    "            query_emb = normalize(image_emb)\n",
    "\n",
    "    # Score against both sets of embeddings\n",
    "    scores_text  = (query_emb @ text_embs.T).squeeze()\n",
    "    scores_image = (query_emb @ image_embs.T).squeeze()\n",
    "\n",
    "    # Combine scores\n",
    "    combined_scores = (scores_text + scores_image) / 2\n",
    "\n",
    "    # Top K results\n",
    "    topk_indices = torch.topk(combined_scores, k=k).indices\n",
    "    top_scores = combined_scores[topk_indices].cpu().numpy()\n",
    "    top_items = df.iloc[topk_indices.cpu().numpy()]\n",
    "\n",
    "    return top_items, top_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450eaf2-a575-4011-a3b8-4f52ffe92403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(text_embs, \"models/catalog_text_embs.pt\")\n",
    "torch.save(image_embs, \"models/catalog_image_embs.pt\")\n",
    "\n",
    "text_embs  = normalize(torch.load(\"models/catalog_text_embs.pt\").to(device))\n",
    "image_embs = normalize(torch.load(\"models/catalog_image_embs.pt\").to(device))\n",
    "\n",
    "model = get_peft_model(CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\"), lora_cfg).to(device)\n",
    "model.load_state_dict(torch.load(\"models/finetuned_clip.pt\"))\n",
    "\n",
    "results, scores = unified_query(input_text=\"denim jacket for women\")\n",
    "display(results.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9465da-0851-44fa-95fb-43354470283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text only\n",
    "results, scores = unified_query(input_text=\"minimalist black backpack\")\n",
    "display(results[[\"text\"]])\n",
    "\n",
    "# Image only\n",
    "results, scores = unified_query(input_image_path=\"./query_imgs/backpack.jpg\")\n",
    "display(results[[\"text\"]])\n",
    "\n",
    "# Both text + image\n",
    "results, scores = unified_query(\n",
    "    input_text=\"black backpack for school\",\n",
    "    input_image_path=\"./query_imgs/backpack.jpg\"\n",
    ")\n",
    "display(results[[\"text\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94382adf-996b-4ab0-93b1-1b0c248bb6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
